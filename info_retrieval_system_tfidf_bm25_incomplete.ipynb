{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "info_retrieval_system_tfidf_bm25_incomplete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyQ1TAFdaOVn",
        "colab_type": "text"
      },
      "source": [
        "TF IDF AND BM25 INFORMATION RETRIEVAL SYSTEM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_SmOpsQabW8",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we build a simple information retrieval system using TF-IDF to generate an array of inverted indices, and then using BM25 (Best Match the 25th iteration) to search and index the articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEeLSzxtaaox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va8RB8Jdaf_U",
        "colab_type": "text"
      },
      "source": [
        "1. Extract: here we extract the data from our dataset\n",
        "\n",
        "2. Organize: our organize step allows us to create a dictionary where we store our information of interest, then we generate a new dictionary where we store our inverted indices based off of keywords that we find for each document\n",
        "\n",
        "3. Retrieve: this step is where we run BM25 and generate our list of sorted results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9og7Mb_6aHzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "83566698-0fba-4f0d-cb5b-91a9d1bc7c00"
      },
      "source": [
        "#libraries for getting data and extracting\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#libraries for text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('wordnet') \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "#libraries for keyword extraction with tf-idf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "#libraries for reading and writing files\n",
        "import pickle\n",
        "\n",
        "#libraries for BM25\n",
        "!pip install rank_bm25\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDR8aynzanGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no6x9QV5anKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def getData():\n",
        "#     urls = ['https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/comm_use_subset.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/noncomm_use_subset.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/custom_license.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/biorxiv_medrxiv.tar.gz']\n",
        "\n",
        "#     # Create data directory\n",
        "#     try:\n",
        "#         os.mkdir('./data')\n",
        "#         print('Directory created')\n",
        "#     except FileExistsError:\n",
        "#         print('Directory already exists')\n",
        "\n",
        "#     #Download all files\n",
        "#     for i in range(len(urls)):\n",
        "#         urllib.request.urlretrieve(urls[i], './data/file'+str(i)+'.tar.gz')\n",
        "#         print('Downloaded file '+str(i+1)+'/'+str(len(urls)))\n",
        "#         tar = tarfile.open('./data/file'+str(i)+'.tar.gz')\n",
        "#         tar.extractall('./data')\n",
        "#         tar.close()\n",
        "#         print('Extracted file '+str(i+1)+'/'+str(len(urls)))\n",
        "#         os.remove('./data/file'+str(i)+'.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO2K4Jm0anEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0bPWl_2eBmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/edited_topics_set2.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JK4nvItenU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "6dbf1540-6287-4b1d-e8eb-3818f923059a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>_id</th>\n",
              "      <th>topic</th>\n",
              "      <th>content</th>\n",
              "      <th>processed_content</th>\n",
              "      <th>processed_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5f04e496ef217aae6a201f71</td>\n",
              "      <td>[\"National\"]</td>\n",
              "      <td>[\"The West Bengal government on Tuesday decide...</td>\n",
              "      <td>west bengal govern tuesday decid impos complet...</td>\n",
              "      <td>nation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5f04e498ef217aae6a201f72</td>\n",
              "      <td>[\"Business\"]</td>\n",
              "      <td>[\"The government is weighing the pros and cons...</td>\n",
              "      <td>govern weigh pros con halt import includ china...</td>\n",
              "      <td>busi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5f04e49aef217aae6a201f73</td>\n",
              "      <td>[\"National\"]</td>\n",
              "      <td>[\"The Central Board of Secondary Education (CB...</td>\n",
              "      <td>central board secondari educ cbse slash syllab...</td>\n",
              "      <td>nation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5f04e49def217aae6a201f74</td>\n",
              "      <td>[\"International\"]</td>\n",
              "      <td>[\"The World Health Organization on Tuesday ack...</td>\n",
              "      <td>world health organ tuesday acknowledg emerg ev...</td>\n",
              "      <td>intern</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5f04e49fef217aae6a201f75</td>\n",
              "      <td>[\"International\"]</td>\n",
              "      <td>[\"President Donald Trump on Tuesday formally s...</td>\n",
              "      <td>presid donald trump tuesday formal start withd...</td>\n",
              "      <td>intern</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  processed_topic\n",
              "0           0  ...           nation\n",
              "1           1  ...             busi\n",
              "2           2  ...           nation\n",
              "3           3  ...           intern\n",
              "4           4  ...           intern\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8lV6lSMasVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def preprocess(text):\n",
        "#     #define stopwords\n",
        "#     stop_words = set(stopwords.words(\"english\"))\n",
        "#     #Remove punctuations\n",
        "#     text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "#     #Convert to lowercase\n",
        "#     text = text.lower()\n",
        "#     #remove tags\n",
        "#     text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "#     # remove special characters and digits\n",
        "#     text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "#     ##Convert to list from string\n",
        "#     text = text.split()\n",
        "#     ##Stemming\n",
        "#     ps=PorterStemmer()\n",
        "#     text = [ps.stem(word) for word in text if not word in stop_words]\n",
        "#     #Lemmatisation\n",
        "#     lem = WordNetLemmatizer()\n",
        "#     text = [lem.lemmatize(word) for word in text if not word in  stop_words] \n",
        "#     text = \" \".join(text) \n",
        "    \n",
        "#     return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhT3DFfnawZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWtZP590a0K6",
        "colab_type": "text"
      },
      "source": [
        "extraction step\n",
        "\n",
        "input: None\n",
        "\n",
        "output: a dataframe with all the extracted information\n",
        "\n",
        "For every article, we are collecting its:\n",
        "\n",
        "paper_id\n",
        "title and\n",
        "abstract\n",
        "We store these values in pandas dataframe, which we write to a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuQNLgkbawXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def extract():\n",
        "#     #create our collection locally in the data folder\n",
        "    \n",
        "#     #creating our initial datastructure\n",
        "#     x = {'paper_id':[], 'title':[], 'abstract': []}\n",
        "    \n",
        "#     #Iterate through all files in the data directory\n",
        "#     for subdir, dirs, files in os.walk('./data'):\n",
        "#         for file in tqdm(files):\n",
        "#             with open(os.path.join(subdir, file)) as f:\n",
        "#                 data = json.load(f)\n",
        "                \n",
        "#                #Append paper ID to list\n",
        "#                 x['paper_id'].append(data['paper_id'])\n",
        "#                #Append article title to list & preprocess the text\n",
        "#                 x['title'].append((data['metadata']['title']))\n",
        "                \n",
        "#                 #Append abstract text content values only to abstract list & preprocess the text\n",
        "#                 abstract = \"\"\n",
        "#                 for paragraph in data['abstract']:\n",
        "#                     abstract += paragraph['text']\n",
        "#                     abstract += '\\n'\n",
        "#                 #if json file no abstract in file, set the body text as the abstract (happens rarely, but often enough that this edge case matters)\n",
        "#                 if abstract == \"\": \n",
        "#                     for paragraph in data['body_text']:\n",
        "#                         abstract += paragraph['text']\n",
        "#                         abstract += '\\n'\n",
        "#                 x['abstract'].append(preprocess(abstract))\n",
        "                \n",
        "#     #Create Pandas dataframe & write to pickle file\n",
        "#     df = pd.DataFrame.from_dict(x, orient='index')\n",
        "#     df = df.transpose()\n",
        "#     pickle.dump( df, open( \"full_data_processed_FINAL.p\", \"wb\" ) )\n",
        "#     return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkmyBofla6to",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw41xJ7Fa91-",
        "colab_type": "text"
      },
      "source": [
        "ORGANIZE\n",
        "\n",
        "sort coordinate matrix\n",
        "\n",
        "input: tf_idf coordinate representation\n",
        "\n",
        "output: tf_idf items sorted in descending order -- so things with highest scores at the top!\n",
        "\n",
        "Function for sorting tf_idf in descending order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1vtbaySa69Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x.processed_content, x.processed_topic), reverse=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCbD3SuebF4h",
        "colab_type": "text"
      },
      "source": [
        "get top n words with highest tf-idf scores\n",
        "\n",
        "input:\n",
        "\n",
        "    1. feature_names = vocabulary\n",
        "    2. sorted_items = tf-idf vectors sorted in descending order\n",
        "    3. topN = # of keywords you would like extract from text\n",
        "\n",
        "output: dictionary of topN # words with highest tf-idf scores in the text (key) and their corresponding tf-idf scores (value)\n",
        "\n",
        "Gets keyword names and their tf-idf scores of topN items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egZQS80xa66E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_topn_from_vector(feature_names, sorted_items, topN):\n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topN]\n",
        " \n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        " \n",
        "    #create a tuples of feature,score\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    return results"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dy1juzJbPds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBkKJsj5bdwc",
        "colab_type": "text"
      },
      "source": [
        "getting abstract keywords\n",
        "input:\n",
        "\n",
        "    1. entry = row in the article dataframe, which represents one article\n",
        "    2. cv = CountVectorizer, from sklearn.feature_extraction.text\n",
        "    3. X = vector that represents the CountVectorizer fit to the corpus\n",
        "    4. tfidf_transformer = object that holds our tf_idf data -- again, fit to our corpus\n",
        "    5. feature_names = vocabulary\n",
        "    6. topN = # of keywords we'd like to extract from the abstract\n",
        "\n",
        "output: the topN keywords from the abstract\n",
        "\n",
        "Extracts the topN keywords from the abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9bpmdR9bPhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAbstractKeywords(entry, cv, X, tfidf_transformer, feature_names, topN):\n",
        "    abstract = entry['abstract']\n",
        "    \n",
        "    #first check that abstract is full\n",
        "    if type(abstract) == float:\n",
        "        return []\n",
        " \n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([abstract])) \n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "    #extract only the topN # items\n",
        "    keywords_dict=extract_topn_from_vector(feature_names,sorted_items,topN)\n",
        "    #just want words themselves, so only need keys of the dictionary\n",
        "    keywords = list(keywords_dict.keys()) \n",
        "     \n",
        "    return keywords"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLVJQgMmbkvI",
        "colab_type": "text"
      },
      "source": [
        "getting title keywords\n",
        "\n",
        "input: entry = row in the article dataframe, which represents one article\n",
        "\n",
        "output: list of all the words in the title\n",
        "\n",
        "Assumed that if a word is in the title of the article, it must be important to the article and treated like a keyword. Thus, this method just extracts all the words from the (already processed) title."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvwFp4wFbPbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTitleKeywords(entry):\n",
        "    content = entry['contents']  \n",
        "    content = preprocess(content)\n",
        "    #first check that the content of that entry is full\n",
        "    if type(content) == float:\n",
        "        return []\n",
        "    \n",
        "    keywords_content = content.split(' ')\n",
        "    return keywords_content"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qudGo5woa6qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXnaWIs0brw0",
        "colab_type": "text"
      },
      "source": [
        "getting final keywords\n",
        "input:\n",
        "\n",
        "    1. entry = row in the article dataframe, which represents one article\n",
        "    2. cv = CountVectorizer, from sklearn.feature_extraction.text\n",
        "    3. X = vector that represents the CountVectorizer fit to the corpus\n",
        "    4. tfidf_transformer = object that holds our tf_idf data -- again, fit to our corpus\n",
        "    5. feature_names = vocabulary\n",
        "    6. topN = # of keywords we'd like to extract from the abstract\n",
        "output: list of all keywords for an article -- extracted from both title and abstract!\n",
        "\n",
        "Calls getTitleKeywords() and getAbstractKeywords() and concatenates the two lists, resulting in a final list of keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8dK83Nbbsct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getFinalKeywords(entry, cv, X, tfidf_trans, feature_names, topN):\n",
        "    #get keywords from abstract and title\n",
        "    fromAbstract = getAbstractKeywords(entry, cv, X, tfidf_trans, feature_names, topN)\n",
        "    fromTitle = getTitleKeywords(entry)\n",
        "    #concatenate two lists\n",
        "    finalKeywords = fromAbstract + fromTitle\n",
        "    #convert to set and then back to list to ensure there are no duplicates in list\n",
        "    final_no_duplicates = list(set(finalKeywords))\n",
        "    return final_no_duplicates\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a91fl5R-bwRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzdCdPMgby5T",
        "colab_type": "text"
      },
      "source": [
        "getting corpus\n",
        "\n",
        "input: dataframe that contains the 1) paper_id 2) abstract and 3) article for every article. All text is processed.\n",
        "\n",
        "output: a list that contains every abstracts in the in the article dataframe\n",
        "\n",
        "Creating a corpus, which is a necessary input to our tf_idf step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO_AkYqhbwWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getCorpus(articlesDf):\n",
        "    #creating a new dataframe, abstractDf, of just the abstracts, so that we don't modify the original dataframe, articlesDf\n",
        "    abstractDf = pd.DataFrame(columns = ['abstract'])\n",
        "    #filling abstractDf with the abstract column from articlesDf\n",
        "    abstractDf['abstract'] = articlesDf['abstract']\n",
        "    #converting column of dataframe to a list\n",
        "    corpus = abstractDf['abstract'].to_list()\n",
        "    return corpus\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X15xhsnBbwP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pi6bOfAb6GE",
        "colab_type": "text"
      },
      "source": [
        "adding keywords\n",
        "input:\n",
        "\n",
        "    1. df = dataframe that contains the 1) paper_id 2) title and 3) abstract for every article. All text is processed \n",
        "    2. topN = # of keywords we'd like to extract from the abstract\n",
        "    3. makeFile = boolean, whether you'd like this method to make a pickle file of the output dataframe\n",
        "    4. fileName = what the user would like to name the pickle file\n",
        "\n",
        "output: pandas dataframe that contains the\n",
        "\n",
        "    1. paper_id \n",
        "    2. title \n",
        "    3. abstract and \n",
        "    4. keywords associated with every article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsBx2XPxb6-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addKeywords(df, topN, makeFile, fileName):\n",
        "    #defining stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    #creating following variables that are needed for keyword extract from abstract, using tf-idf methodology,\n",
        "    #all input in getFinalKewords method\n",
        "    corpus = getCorpus(df)\n",
        "    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=1000, ngram_range=(1,1))    \n",
        "    X=cv.fit_transform(corpus)\n",
        "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "    tfidf_transformer.fit(X)\n",
        "    feature_names=cv.get_feature_names()\n",
        "    \n",
        "    #adding keywords article to dataframe\n",
        "    df = df.reindex(columns = ['paper_id', 'title', 'abstract','keywords'])                \n",
        "    #getting keywords for each entry in article dataframe -- using apply to be more efficient\n",
        "    df['keywords'] = df.apply(lambda row: getFinalKeywords(row, cv, X, tfidf_transformer, feature_names, topN), axis=1)\n",
        "\n",
        "    #make pickle file depending on user input\n",
        "    if makeFile == True:\n",
        "        pickle.dump( df, open( fileName, \"wb\" ) )\n",
        "    return df  \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYoAf0W1b-k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Aye7lJcCRr",
        "colab_type": "text"
      },
      "source": [
        "creating inverted indices\n",
        "\n",
        "input: pandas dataframe that contains the 1) paper_id 2) title 3)abstract and 4) keywords associated with every article\n",
        "\n",
        "output: dictionary of inverted indices -- key = word that is a keyword; value = all articles' paper_id's that have word as a keyword\n",
        "\n",
        "Creating an inverted indices dictionary. Will use this when deciding which subset of articles to run our ranking/retrieving algorithm on. It is important that our output is a dictionary because it has constant look up time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8pOhrvHb-r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createInvertedIndices(df):\n",
        "    numEntries = df.shape[0]\n",
        "    invertInd = {}\n",
        "    \n",
        "    for i in range (numEntries):\n",
        "        entry = df.iloc[i]\n",
        "        paper_id = entry['paper_id']    \n",
        "        keywords = entry['keywords']\n",
        "        for k in keywords:\n",
        "            if k not in invertInd:\n",
        "                invertInd[k] = []\n",
        "                invertInd[k].append(paper_id)\n",
        "            else:\n",
        "                invertInd[k].append(paper_id)\n",
        "    return invertInd\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts-bO-ohb-ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# organize step\n",
        "\n",
        "def organize():\n",
        "    df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n",
        "    df_with_keywords = addKeywords(df_without_keywords, 10, False, \"full_data_withKeywords_FINAL.p\")\n",
        "    invertedIndices = createInvertedIndices(df_with_keywords)\n",
        "    pickle.dump( invertedIndices, open( \"invertedIndices_FINAL.p\", \"wb\" ) )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpG5_BP5b-ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvGlqWv9cPv1",
        "colab_type": "text"
      },
      "source": [
        "Processed data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NgH2QD8cQkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getData()\n",
        "# extract()\n",
        "\n",
        "# df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n",
        "# df_with_keywords = addKeywords(df_without_keywords, 10, False, \"full_data_withKeywords_FINAL.p\")\n",
        "# display(df_without_keywords)\n",
        "# display(df_with_keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYMrkmT5cT_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGMrNFtBcW88",
        "colab_type": "text"
      },
      "source": [
        "RETRIEVE\n",
        "\n",
        "getting our subset of articles\n",
        "\n",
        "input: query\n",
        "\n",
        "output: a list of the potential articles that may be of interest, as they have some of the query terms as their keyword(s)\n",
        "\n",
        "Doing this step so we don't have to run our ranking algorithm, BM25, over all ~30,000 articles. Want to identify this subset in order to make our ranking (and therefore retrieving) more efficient!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syBeI3gNcT9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPotentialArticleSubset(query):\n",
        "    #load in inverted indices\n",
        "    invertedIndices = pickle.load(open(\"invertedIndices_FINAL.p\", \"rb\"))\n",
        "    \n",
        "    #preprocess query and split into individual terms\n",
        "    query = preprocess(query)\n",
        "    queryTerms = query.split(' ')\n",
        "    \n",
        "    potentialArticles = []\n",
        "    #concatenate list of potential articles by looping through potential articles for each word in query\n",
        "    for word in queryTerms:\n",
        "        if word in invertedIndices: #so if someone types in nonsensical query term that's not in invertedIndices, still won't break!\n",
        "            someArticles = invertedIndices[word]\n",
        "            potentialArticles = potentialArticles + someArticles\n",
        "            \n",
        "    #convert to set then back to list so there are no repeat articles\n",
        "    potentialArticles = list(set(potentialArticles))\n",
        "    return potentialArticles\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujbIpAtdcbSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3YVAeSychVZ",
        "colab_type": "text"
      },
      "source": [
        "bm25 method\n",
        "\n",
        "input: list of articles, dictionary with all of the documents, weight of the title, weight of the abstract, and the query\n",
        "\n",
        "output: list of ranked articles\n",
        "\n",
        "This is the main information retrieval method implementing Okapi BM25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDPWFJnZcbWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bm25(articles, df_dic, title_w, abstract_w, query):\n",
        "    corpus_title = []\n",
        "    corpus_abstract = []\n",
        "    \n",
        "    for article in articles:\n",
        "        arr = df_dic.get(article)\n",
        "        #title\n",
        "        if type(arr[0]) != float:\n",
        "            preprocessedTitle = preprocess(arr[0])\n",
        "            corpus_title.append(preprocessedTitle)\n",
        "        else:\n",
        "            corpus_title.append(\" \")\n",
        "        \n",
        "        #abstract\n",
        "        if type(arr[1]) != float:\n",
        "            preprocessedAbst = preprocess(arr[1])\n",
        "            corpus_abstract.append(preprocessedAbst)\n",
        "        else:\n",
        "            corpus_abstract.append(\" \")\n",
        "            \n",
        "    query = preprocess(query)\n",
        "    \n",
        "    tokenized_query = query.split(\" \")\n",
        "    \n",
        "    tokenized_corpus_title = [doc.split(\" \") for doc in corpus_title]\n",
        "    tokenized_corpus_abstract = [doc.split(\" \") for doc in corpus_abstract]\n",
        "    \n",
        "    #running bm25 on titles\n",
        "    bm25_title = BM25Okapi(tokenized_corpus_title)\n",
        "    doc_scores_titles = bm25_title.get_scores(tokenized_query)\n",
        "    #weighting array\n",
        "    doc_scores_titles = np.array(doc_scores_titles)\n",
        "    doc_scores_titles = doc_scores_titles**title_w\n",
        "    \n",
        "    #running bm25 on abstracts\n",
        "    bm25_abstract = BM25Okapi(tokenized_corpus_abstract)\n",
        "    doc_scores_abstracts = bm25_abstract.get_scores(tokenized_query)\n",
        "    #weighting\n",
        "    doc_scores_abstracts = np.array(doc_scores_abstracts)\n",
        "    doc_scores_abstracts = doc_scores_abstracts ** abstract_w\n",
        "    \n",
        "    #summing up the two different scores\n",
        "    doc_scores = np.add(doc_scores_abstracts,doc_scores_titles)\n",
        "    \n",
        "    #creating a dictionary with the scores\n",
        "    score_dict = dict(zip(articles, doc_scores))\n",
        "    \n",
        "    #creating list of ranked documents high to low\n",
        "    doc_ranking = sorted(score_dict, key=score_dict.get, reverse = True)\n",
        "    \n",
        "    #get top 100\n",
        "    doc_ranking = doc_ranking[0:100]\n",
        "    \n",
        "    for i in range(len(doc_ranking)):\n",
        "        dic_entry = df_dic.get(doc_ranking[i])\n",
        "        doc_ranking[i] = dic_entry[0]\n",
        "    \n",
        "    return doc_ranking"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR5oPzt0cmyW",
        "colab_type": "text"
      },
      "source": [
        "RETRIEVAL STEP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s38Hb1ncbQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve(queries):\n",
        "    #performing information retrieval\n",
        "    df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n",
        "    df_dic = df_without_keywords.set_index('paper_id').T.to_dict('list')\n",
        "    results = []\n",
        "    for q in queries:\n",
        "        articles = getPotentialArticleSubset(q)\n",
        "        result = bm25(articles,df_dic,1,2,q)\n",
        "        results.append(result)\n",
        "\n",
        "    #Output results\n",
        "    for query in range(len(results)):\n",
        "        for rank in range(len(results[query])):\n",
        "            print(str(query+1)+'\\t'+str(rank+1)+'\\t'+str(results[query][rank]))\n",
        "            "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaFTsAT6cs7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev51zCHtcv8c",
        "colab_type": "text"
      },
      "source": [
        "Putting it all together\n",
        "\n",
        "this will give us the top 100 articles for each query sorted by BM25 rank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxcmwqWCcs_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getData()\n",
        "extract()\n",
        "organize()\n",
        "q = ['coronavirus origin',\n",
        "'coronavirus response to weather changes',\n",
        "'coronavirus immunity']\n",
        "retrieve(q)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqdaJ-80cs5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}