{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intelligent_doc_search_unrun.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6r8YDzJ0S6v",
        "colab_type": "text"
      },
      "source": [
        "Search.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xamIQqc9z_-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "This algorithms have been taken from the paper:\n",
        "Trotmam et al, Improvements to BM25 and Language Models Examined\n",
        "\"\"\"\n",
        "class search:\n",
        "    def __init__(self, corpus):\n",
        "        self.corpus_size = len(corpus)\n",
        "        self.avgdl = 0\n",
        "        self.doc_freqs = []\n",
        "        self.idf = {}\n",
        "        self.doc_len = []\n",
        "\n",
        "        nd = self._initialize(corpus)\n",
        "        self._calc_idf(nd)\n",
        "\n",
        "    def _initialize(self, corpus):\n",
        "        nd = {}  # word -> number of documents with word\n",
        "        num_doc = 0\n",
        "        for document in corpus:\n",
        "            self.doc_len.append(len(document))\n",
        "            num_doc += len(document)\n",
        "\n",
        "            frequencies = {}\n",
        "            for word in document:\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 0\n",
        "                frequencies[word] += 1\n",
        "            self.doc_freqs.append(frequencies)\n",
        "\n",
        "            for word, freq in frequencies.items():\n",
        "                if word not in nd:\n",
        "                    nd[word] = 0\n",
        "                nd[word] += 1\n",
        "\n",
        "        self.avgdl = num_doc / self.corpus_size\n",
        "        return nd\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_top_n(self, query, documents, n=5):\n",
        "\n",
        "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
        "\n",
        "        scores = self.get_scores(query)\n",
        "        top_n = np.argsort(scores)[::-1][:n]\n",
        "        return top_n,[documents[i] for i in top_n]\n",
        "\n",
        "\n",
        "class search_by_BM25(search):\n",
        "    def __init__(self, corpus, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        \"\"\"\n",
        "        Calculates frequencies of terms in documents and in corpus.\n",
        "        This algorithm sets a floor on the idf values to eps * average_idf\n",
        "        \"\"\"\n",
        "        # collect idf sum to calculate an average idf for epsilon value\n",
        "        idf_sum = 0\n",
        "        # collect words with negative idf to set them a special epsilon value.\n",
        "        # idf can be negative if word is contained in more than half of documents\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        \"\"\"\n",
        "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
        "        this algorithm also adds a floor to the idf value of epsilon.\n",
        "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
        "        :param query:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1JsLYJS0jcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqT7r26f0Vy7",
        "colab_type": "text"
      },
      "source": [
        "Auto-tagging-Script.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqD3W00D0WJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this file include source code of this beautiful repositry --> https://github.com/acrosson/nlp/tree/master/subject_extraction\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import sqlite3\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
        "import pickle\n",
        "\n",
        "class SubjectTrigramTagger(object):\n",
        "    \"\"\" Creates an instance of NLTKs TrigramTagger with a backoff\n",
        "    tagger of a bigram tagger a unigram tagger and a default tagger that sets\n",
        "    all words to nouns (NN)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_sents):\n",
        "        \"\"\"\n",
        "        train_sents: trained sentences which have already been tagged.\n",
        "                Currently using Brown, conll2000, and TreeBank corpuses\n",
        "        \"\"\"\n",
        "\n",
        "        t0 = DefaultTagger('NN')\n",
        "        t1 = UnigramTagger(train_sents, backoff=t0)\n",
        "        t2 = BigramTagger(train_sents, backoff=t1)\n",
        "        self.tagger = TrigramTagger(train_sents, backoff=t2)\n",
        "\n",
        "    def tag(self, tokens):\n",
        "        return self.tagger.tag(tokens)\n",
        "\n",
        "class AutoTags:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "        self.VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "    def clean_document(self, document):\n",
        "        \"\"\"Remove enronious characters. Extra whitespace and stop words\"\"\"\n",
        "        document = re.sub('[^A-Za-z .-]+', ' ', document)\n",
        "        document = ' '.join(document.split())\n",
        "        document = ' '.join([i for i in document.split() if i not in STOPWORDS])\n",
        "        return document\n",
        "\n",
        "\n",
        "    def tokenize_sentences(self, document):\n",
        "        sentences = sent_tokenize(document)\n",
        "        sentences = [word_tokenize(sent) for sent in sentences]\n",
        "        return sentences\n",
        "\n",
        "\n",
        "    def get_entities(self, document):\n",
        "        \"\"\"Returns Named Entities using NLTK Chunking\"\"\"\n",
        "        entities = []\n",
        "        sentences = self.tokenize_sentences(document)\n",
        "\n",
        "        # Part of Speech Tagging\n",
        "        sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
        "        for tagged_sentence in sentences:\n",
        "            for chunk in nltk.ne_chunk(tagged_sentence):\n",
        "                if type(chunk) == nltk.tree.Tree:\n",
        "                    entities.append(' '.join([c[0] for c in chunk]).lower())\n",
        "        return entities\n",
        "\n",
        "\n",
        "    def word_freq_dist(self, document):\n",
        "        \"\"\"Returns a word count frequency distribution\"\"\"\n",
        "        words = nltk.tokenize.word_tokenize(document)\n",
        "        words = [word.lower() for word in words if word not in STOPWORDS]\n",
        "        fdist = nltk.FreqDist(words)\n",
        "        return fdist\n",
        "\n",
        "    def extract_subject(self, document):\n",
        "        # Get most frequent Nouns\n",
        "        fdist = self.word_freq_dist(document)\n",
        "        most_freq_nouns = [w for w, c in fdist.most_common(10)\n",
        "                           if nltk.pos_tag([w])[0][1] in self.NOUNS]\n",
        "\n",
        "        # Get Top 10 entities\n",
        "        entities = self.get_entities(document)\n",
        "        entities = list(set(entities))\n",
        "        top_10_entities = [w for w, c in nltk.FreqDist(entities).most_common(10)]\n",
        "\n",
        "        # Get the subject noun by looking at the intersection of top 10 entities\n",
        "        # and most frequent nouns. It takes the first element in the list\n",
        "        subject_nouns = [entity for entity in top_10_entities\n",
        "                         if entity.split()[0] in most_freq_nouns]\n",
        "        if len(subject_nouns) != 0:\n",
        "            return subject_nouns[0]\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "    # def trained_tagger(self,existing=False):\n",
        "    #     \"\"\"Returns a trained trigram tagger\n",
        "    #     existing : set to True if already trained tagger has been pickled\n",
        "    #     \"\"\"\n",
        "    #     if existing:\n",
        "            # trigram_tagger = pickle.load(\n",
        "            #     open(r'DataBase/trained_tagger.pkl', 'rb'))\n",
        "            # return trigram_tagger\n",
        "\n",
        "    #     # Aggregate trained sentences for N-Gram Taggers\n",
        "    #     train_sents = nltk.corpus.brown.tagged_sents()\n",
        "    #     train_sents += nltk.corpus.conll2000.tagged_sents()\n",
        "    #     train_sents += nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "    #     # Create instance of SubjectTrigramTagger and persist instance of it\n",
        "    #     trigram_tagger = SubjectTrigramTagger(train_sents)\n",
        "    #     pickle.dump(trigram_tagger, open(r'DataBase/trained_tagger.pkl', 'wb'))\n",
        "\n",
        "    #     return trigram_tagger\n",
        "\n",
        "    def merge_multi_word_subject(self, sentences, subject):\n",
        "        \"\"\"Merges multi word subjects into one single token\n",
        "        ex. [('steve', 'NN', ('jobs', 'NN')] -> [('steve jobs', 'NN')]\n",
        "        \"\"\"\n",
        "        if len(subject.split()) == 1:\n",
        "            return sentences\n",
        "        subject_lst = subject.split()\n",
        "        sentences_lower = [[word.lower() for word in sentence]\n",
        "                           for sentence in sentences]\n",
        "        for i, sent in enumerate(sentences_lower):\n",
        "            if subject_lst[0] in sent:\n",
        "                for j, token in enumerate(sent):\n",
        "                    start = subject_lst[0] == token\n",
        "                    exists = subject_lst == sent[j:j + len(subject_lst)]\n",
        "                    if start and exists:\n",
        "                        del sentences[i][j + 1:j + len(subject_lst)]\n",
        "                        sentences[i][j] = subject\n",
        "        return sentences\n",
        "\n",
        "    def tag_sentences(self, subject, document):\n",
        "        \"\"\"Returns tagged sentences using POS tagging\"\"\"\n",
        "        trigram_tagger = pickle.load(open(r'DataBase/trained_tagger.pkl', 'rb'))\n",
        "\n",
        "        # Tokenize Sentences and words\n",
        "        sentences = self.tokenize_sentences(document)\n",
        "        self.merge_multi_word_subject(sentences, subject)\n",
        "\n",
        "        # Filter out sentences where subject is not present\n",
        "        sentences = [sentence for sentence in sentences if subject in\n",
        "                     [word.lower() for word in sentence]]\n",
        "\n",
        "        # Tag each sentence\n",
        "        tagged_sents = [trigram_tagger.tag(sent) for sent in sentences]\n",
        "        return tagged_sents\n",
        "\n",
        "    def get_svo(self, sentence, subject):\n",
        "        \"\"\"Returns a dictionary containing:\n",
        "        subject : the subject determined earlier\n",
        "        action : the action verb of particular related to the subject\n",
        "        object : the object the action is referring to\n",
        "        phrase : list of token, tag pairs for that lie within the indexes of\n",
        "                    the variables above\n",
        "        \"\"\"\n",
        "        subject_idx = next((i for i, v in enumerate(sentence)\n",
        "                            if v[0].lower() == subject), None)\n",
        "        data = {'subject': subject}\n",
        "        for i in range(subject_idx, len(sentence)):\n",
        "            found_action = False\n",
        "            for j, (token, tag) in enumerate(sentence[i + 1:]):\n",
        "                if tag in self.VERBS:\n",
        "                    data['action'] = token\n",
        "                    found_action = True\n",
        "                if tag in self.NOUNS and found_action == True:\n",
        "                    data['object'] = token\n",
        "                    data['phrase'] = sentence[i: i + j + 2]\n",
        "                    return data\n",
        "        return {}\n",
        "\n",
        "\n",
        "    def get_auto_tags_from_document(self, text, doc_id):\n",
        "        document = text\n",
        "        document = self.clean_document(document)\n",
        "\n",
        "        entities = self.get_entities(document)\n",
        "\n",
        "        if len(entities) == 0:\n",
        "            auto_tags = word_tokenize(document.lower())\n",
        "            auto_tags = list(set(auto_tags))\n",
        "            # cleaning the auto tags futher for better search\n",
        "            # auto_tags = clean_auto_tags(auto_tags)\n",
        "            auto_tags = [word for word in auto_tags if word not in STOPWORDS and len(word) > 2]\n",
        "            return str(auto_tags), \"\"\n",
        "\n",
        "        entities = list(set(entities))\n",
        "\n",
        "        # final list for storing automatic generated tags (auto_tags)\n",
        "        auto_tags = entities\n",
        "\n",
        "        subject = self.extract_subject(document)\n",
        "        if subject == \"\":\n",
        "            # If there is no subject then tokenize the summary to get better tags than tokenizing the whole documents\n",
        "\n",
        "            # conn = sqlite3.connect(\"Document_finder_db2.db\")\n",
        "            # c = conn.cursor()\n",
        "            # c.execute(f\"SELECT summary FROM document_summary where doc_id='{doc_id}' \")\n",
        "            # temp_summary = c.fetchone()\n",
        "            # conn.close()\n",
        "            #\n",
        "            # temp_tags = list(set(word_tokenize(temp_summary[0])))\n",
        "            # auto_tags += temp_tags\n",
        "            # auto_tags = [word for word in auto_tags if word not in STOPWORDS and len(word) > 2]\n",
        "            # return str(auto_tags), \"\"\n",
        "            return str(auto_tags), \"\"\n",
        "\n",
        "        tagged_sents = self.tag_sentences(subject, document)\n",
        "\n",
        "        svos = [self.get_svo(sentence, subject)\n",
        "                for sentence in tagged_sents]\n",
        "        svos_list = []\n",
        "        for svo in svos:\n",
        "            if svo:\n",
        "                svo_word = svo[\"subject\"] + \" \" + svo[\"action\"] + \" \" + svo[\"object\"]\n",
        "                svos_list.append(svo_word)\n",
        "\n",
        "                auto_tags.append(svo_word)\n",
        "                for num in svo[\"phrase\"]:\n",
        "                    temp_word = num[0].lower()\n",
        "                    if temp_word not in entities and temp_word not in STOPWORDS:\n",
        "                        auto_tags.append(temp_word)\n",
        "\n",
        "        auto_tags = [word for word in auto_tags if word not in STOPWORDS and len(word) > 2]\n",
        "        return str(auto_tags), str(svos_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_MeYrs_0pOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CNdliDn0puL",
        "colab_type": "text"
      },
      "source": [
        "Final fulldb scrapit.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK2ejzlg0pCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import re\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "\n",
        "import textract\n",
        "\n",
        "\n",
        "def writeTofile(data, filename):\n",
        "    # Convert binary data to proper format and write it on Hard Disk\n",
        "    with open(filename, 'wb') as file:\n",
        "        file.write(data)\n",
        "    print(\"Stored blob data into: \", filename, \"\\n\")\n",
        "\n",
        "valid_extensions = {'docx', 'pptx', 'txt', 'pdf'}\n",
        "\n",
        "class PreProcess:\n",
        "\n",
        "    def __init__(self, file):\n",
        "        self.file = file\n",
        "\n",
        "    def check_extension(self):\n",
        "        if self.file.split('.')[-1] in valid_extensions:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def get_extension(self):\n",
        "        return self.file.split('.')[-1]\n",
        "\n",
        "\n",
        "    def get_text_from_docx_document(self):\n",
        "        try:\n",
        "            doc = Document(self.file)\n",
        "            temp = ''\n",
        "            for para in doc.paragraphs:\n",
        "                temp += para.text\n",
        "            return temp\n",
        "        except Exception:\n",
        "            print('Raising......')\n",
        "            raise Exception\n",
        "\n",
        "    #     text = textract.process(file)\n",
        "    #     text = str(text)[2:]\n",
        "    #     return text\n",
        "\n",
        "    def get_text_from_pdf_document(self):\n",
        "        file_obj = open(self.file, \"rb\")\n",
        "        pdf_reader = PyPDF2.PdfFileReader(file_obj)\n",
        "        page_numbers = pdf_reader.numPages\n",
        "        temp = ''\n",
        "\n",
        "        for i in range(page_numbers):\n",
        "            page_obj = pdf_reader.getPage(i)\n",
        "            temp += page_obj.extractText()\n",
        "        file_obj.close()\n",
        "        return temp\n",
        "\n",
        "\n",
        "    def get_text_from_txt_document(self):\n",
        "        #     text = textract.process(file)\n",
        "        #     text = str(text)[2:]\n",
        "        try:\n",
        "            f = open(self.file, \"r\")\n",
        "            temp = f.read()\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            #         print(\"\\n\\nI am in except block\\n\\n\")\n",
        "            try:\n",
        "                f = open(self.file, \"r\", encoding=\"utf-8\")\n",
        "                temp = f.read()\n",
        "            except:\n",
        "                print(\"Sorry! can't decode encodings!\")\n",
        "                raise Exception(\"Sorry! can't decode bytes\")\n",
        "        # except Exception:\n",
        "        #     print('Another exception occured')\n",
        "        #     raise Exception\n",
        "        finally:\n",
        "            f.close()\n",
        "            return temp\n",
        "\n",
        "\n",
        "\n",
        "    def get_text_from_pptx_document(self):\n",
        "        text = textract.process(self.file)\n",
        "        text = str(text)[2:]\n",
        "        return text\n",
        "\n",
        "\n",
        "    def remove_escape_sequences(self, text):\n",
        "        pattern = r\"\\\\[a-z]\"\n",
        "        text = re.sub(pattern, \" \", text)\n",
        "        return text\n",
        "\n",
        "\n",
        "def load_word_embeddings():\n",
        "    global word_embeddings\n",
        "    word_embeddings = {}\n",
        "    f = open(r'DataBase/glove.6B.100d.txt', encoding=\"utf-8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        word_embeddings[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def cleaning_for_summarization(text):\n",
        "    pattern = r\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\"\n",
        "    text = re.sub(pattern, \" \", text)\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    #     for j in range(len(sentences)):\n",
        "    #         sentences[j] = re.sub(\"[^a-zA-Z]\",\" \",sentences[j])\n",
        "\n",
        "    clean_sentences = sentences\n",
        "\n",
        "    for j in range(len(clean_sentences)):\n",
        "        clean_sentences[j] = word_tokenize(clean_sentences[j])\n",
        "\n",
        "    return clean_sentences\n",
        "\n",
        "\n",
        "def get_summary(text, word_embeddings):\n",
        "    tokenized_sent = cleaning_for_summarization(text)\n",
        "\n",
        "    sentence_vectors = []\n",
        "    for i in tokenized_sent:\n",
        "        if len(i) != 0:\n",
        "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i]) / (len(i) + 0.001)\n",
        "        else:\n",
        "            v = np.zeros((100,))\n",
        "        sentence_vectors.append(v)\n",
        "\n",
        "    # similarity matrix\n",
        "    sim_mat = np.zeros([len(tokenized_sent), len(tokenized_sent)])\n",
        "\n",
        "    for i in range(len(tokenized_sent)):\n",
        "        for j in range(len(tokenized_sent)):\n",
        "            if i != j:\n",
        "                sim_mat[i][j] = \\\n",
        "                cosine_similarity(sentence_vectors[i].reshape(1, 100), sentence_vectors[j].reshape(1, 100))[0, 0]\n",
        "\n",
        "    nx_graph = nx.from_numpy_array(sim_mat)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(tokenized_sent)), reverse=True)\n",
        "    summarize_text = []\n",
        "    if len(ranked_sentence) == 1:\n",
        "        summarize_text.append(\" \".join(ranked_sentence[0][1]))\n",
        "    elif len(ranked_sentence) == 0:\n",
        "        summarize_text = []\n",
        "    else:\n",
        "        for i in range(2):\n",
        "            summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    if len(\". \".join(summarize_text)) > 1400:\n",
        "        summary = summarize_text[0]\n",
        "    else:\n",
        "        summary = \". \".join(summarize_text)\n",
        "\n",
        "    return summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4mCOut802KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lLnRx1W01cH",
        "colab_type": "text"
      },
      "source": [
        "Ready for search.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVuy1hSR01KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3\n",
        "import pickle\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import re\n",
        "\n",
        "class MakeDataForSearch:\n",
        "    def __init__(self, data, titles,summaries,documents,svos):\n",
        "        self.data, self.titles, self.summaries, self.documents, self.svos = self.get_all_texts_summary_titles_documents()\n",
        "\n",
        "    def fetch_all_texts(self):\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "        c.execute(\"SELECT text from document_info\")\n",
        "        tup = c.fetchall()\n",
        "        conn.close()\n",
        "        return tup\n",
        "\n",
        "    def fetch_all_titles(self):\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "        c.execute(\"SELECT title from document_info\")\n",
        "        tup = c.fetchall()\n",
        "        conn.close()\n",
        "        return tup\n",
        "    def fetch_all_summary(self):\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "        c.execute(\"SELECT summary from document_summary\")\n",
        "        tup = c.fetchall()\n",
        "        conn.close()\n",
        "        return tup\n",
        "    def fetch_all_svos(self):\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "        c.execute(\"SELECT auto_tags from document_tags\")\n",
        "        tup = c.fetchall()\n",
        "        conn.close()\n",
        "        return tup\n",
        "    def fetch_all_documentsWithExtensions(self):\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "        c.execute(\"SELECT document,extension from document_info\")\n",
        "        tup = c.fetchall()\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        return tup\n",
        "\n",
        "    def get_all_texts_summary_titles_documents(self):\n",
        "        svos_file = []\n",
        "        texts = self.fetch_all_texts()\n",
        "        titles = self.fetch_all_titles()\n",
        "        summaries = self.fetch_all_summary()\n",
        "        svos = self.fetch_all_svos()\n",
        "        tup = self.fetch_all_documentsWithExtensions()\n",
        "\n",
        "        data_file = [text[0] for text in texts]\n",
        "        title_file = [title[0] for title in titles]\n",
        "        summary_file = [summary[0] for summary in summaries]\n",
        "\n",
        "        for i in range(len(svos)):\n",
        "            lst = svos[i][0][1:-1].split(\"'\")\n",
        "            lst = [word for word in lst if word not in STOPWORDS and len(word) > 2]\n",
        "            svo = [word for word in lst if len(word.split()) > 1]\n",
        "            svos_file.append(svo)\n",
        "\n",
        "        blob_list = [tup[k][0] for k in range(len(tup))]\n",
        "        entension_list = [tup[k][1] for k in range(len(tup))]\n",
        "        index_list = [i for i in range(len(tup))]\n",
        "\n",
        "        dictionary = {k: {\"document\": x, \"extension\": y} for (k, x, y) in zip(index_list, blob_list, entension_list)}\n",
        "\n",
        "        return data_file, title_file, summary_file, dictionary, svos_file\n",
        "\n",
        "\n",
        "def get_corpus(text):\n",
        "    \"\"\"\n",
        "    Function to clean text of websites, email addresess and any punctuation\n",
        "    We also lower case the text\n",
        "    \"\"\"\n",
        "    pattern = r\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\"\n",
        "    text = str(text)\n",
        "    text = re.sub(pattern, \" \", text)\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "    # return text\n",
        "    text = [word for word in text if word not in STOPWORDS]\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word) for word in text if len(word) > 2]\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word, pos='v') for word in lemmed]\n",
        "    return lemmed\n",
        "\n",
        "\n",
        "def get_latest_text_title():\n",
        "    conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT title,text from document_info where rowid = (SELECT MAX(rowid) FROM document_info)\")\n",
        "    tup = c.fetchall()\n",
        "    conn.close()\n",
        "    return tup\n",
        "\n",
        "def get_latest_tags():\n",
        "    conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT manual_tags,auto_tags from document_tags where rowid = (SELECT MAX(rowid) FROM document_tags)\")\n",
        "    tup = c.fetchone()\n",
        "    conn.close()\n",
        "    return tup\n",
        "\n",
        "\n",
        "# corpus = []\n",
        "# for i in range(len(data)):\n",
        "#     corpus.append(apply_all(titles[i]) + apply_all(data[i]))\n",
        "\n",
        "# pickle.dump(corpus, open(\"corpus_file.pkl\",\"wb\"))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "def maintaining_all_files():\n",
        "    data = []\n",
        "    titles = []\n",
        "    summaries = []\n",
        "    documents = {}\n",
        "    svos = []\n",
        "    obj = MakeDataForSearch(data, titles, summaries, documents, svos)\n",
        "\n",
        "    pickle.dump(obj.data, open(r\"DataBase/data_file.pkl\", \"wb\"))\n",
        "    pickle.dump(obj.titles, open(r\"DataBase/title_file.pkl\", \"wb\"))\n",
        "    pickle.dump(obj.summaries, open(r\"DataBase/summary_file.pkl\", \"wb\"))\n",
        "    pickle.dump(obj.documents, open(r\"DataBase/document_file.pkl\", \"wb\"))\n",
        "    pickle.dump(obj.svos, open(r\"DataBase/svos_file.pkl\", \"wb\"))\n",
        "\n",
        "    print(\"Files is updated\")\n",
        "\n",
        "    # appending to the main corpus\n",
        "    corpus = pickle.load(open(r\"DataBase/corpus_file.pkl\", \"rb\"))\n",
        "\n",
        "    temp = get_latest_text_title()\n",
        "    text = temp[0][1]\n",
        "    title = temp[0][0]\n",
        "    corpus.append(get_corpus(text)+get_corpus(title))\n",
        "\n",
        "    pickle.dump(corpus, open(r\"DataBase/corpus_file.pkl\", \"wb\"))\n",
        "\n",
        "    # appending to the tags corpus\n",
        "    final_auto_tags = pickle.load(open(r\"DataBase/tags_pickle.pkl\", \"rb\"))\n",
        "\n",
        "    tup = get_latest_tags()\n",
        "    temp_auto_tags = tup[1][1:-1].split(\"'\")\n",
        "    temp_manual_tags = tup[0][1:-1].split(\"'\")\n",
        "\n",
        "    auto_tags = temp_manual_tags + temp_auto_tags\n",
        "    auto_tags = [word for word in auto_tags if word not in STOPWORDS and len(word) > 2]\n",
        "\n",
        "    final_auto_tags.append(auto_tags)\n",
        "\n",
        "    pickle.dump(final_auto_tags, open(r\"DataBase/tags_pickle.pkl\", \"wb\"))\n",
        "\n",
        "    # appending to title corpus\n",
        "    title_corpus = pickle.load(open(r\"DataBase/title_corpus.pkl\", \"rb\"))\n",
        "    title_corpus.append(get_corpus(title))\n",
        "\n",
        "    pickle.dump(title_corpus, open(r\"DataBase/title_corpus.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "    print(\"corpus is updated\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCk82QPO1DPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfXIbARM1DTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-T4vHY1I1Q",
        "colab_type": "text"
      },
      "source": [
        "Search by default feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAjzRTvL1DNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Search import search_by_BM25\n",
        "\n",
        "import nltk\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import FreqDist\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def clean_query(query):\n",
        "    '''\n",
        "    Function to perform lemmatization and cleaning on query\n",
        "    '''\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word) for word in word_tokenize(query) if word not in STOPWORDS]\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word, pos='v') for word in lemmed]\n",
        "    lemmed = list(set(lemmed))\n",
        "\n",
        "    # applying spell checker on tags\n",
        "    spell = SpellChecker()\n",
        "    misspelled = spell.unknown(lemmed)\n",
        "    if len(misspelled) == 0:\n",
        "        return lemmed\n",
        "    else:\n",
        "        correct_words = list(set(lemmed) - misspelled)\n",
        "        correction = []\n",
        "\n",
        "        for word in misspelled:\n",
        "            # Get the one `most likely` answer\n",
        "            correction.append(spell.correction(word))\n",
        "        new_query = query\n",
        "        for i in range(len(correction)):\n",
        "            new_query = new_query.replace(list(misspelled)[i], correction[i])\n",
        "\n",
        "        # cleaned auto_tags\n",
        "        lemmed = correct_words + correction\n",
        "        print(f\"Searching for {new_query} instead of {query}\")\n",
        "        return lemmed\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = pickle.load(open(r\"DataBase/data_file.pkl\", \"rb\"))\n",
        "    titles = pickle.load(open(r\"DataBase/title_file.pkl\", \"rb\"))\n",
        "\n",
        "    option = input(\"Enter option of search\")\n",
        "\n",
        "    if option == 'default search':\n",
        "        corpus = pickle.load(open(r\"DataBase/corpus_file.pkl\", \"rb\"))\n",
        "    elif option == 'tag search':\n",
        "        corpus = pickle.load(open(r\"DataBase/tags_pickle.pkl\", \"rb\"))\n",
        "    elif option == 'title search':\n",
        "        corpus = pickle.load(open(r\"DataBase/title_corpus.pkl\", \"rb\"))\n",
        "    else:\n",
        "        print('Not valid option')\n",
        "\n",
        "    bm25 = search_by_BM25(corpus)\n",
        "    query = input(\"Enter query\")\n",
        "    tokenized_query = clean_query(query.lower())\n",
        "\n",
        "    indexes, results = bm25.get_top_n(tokenized_query, data, n=20)\n",
        "    results_titles = []\n",
        "    for i in indexes:\n",
        "        results_titles.append(titles[i])\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f\"Title_{i}: {results_titles[i]}\")\n",
        "        print(f\"\\nText_{i}: {results[i]}\")\n",
        "        print('\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy7Jhm6Z1MJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_k8rNa41bJE",
        "colab_type": "text"
      },
      "source": [
        "app.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE-TZ27I1MGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask, flash, request, redirect, render_template\n",
        "from flask_wtf import FlaskForm\n",
        "from wtforms import StringField, SubmitField, FileField\n",
        "import os\n",
        "import urllib.request\n",
        "import sqlite3\n",
        "\n",
        "from werkzeug.utils import secure_filename\n",
        "import re\n",
        "from Search import search_by_BM25\n",
        "\n",
        "import pickle\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "import random\n",
        "from docx import Document\n",
        "from auto_tagging_script import AutoTags\n",
        "\n",
        "from final_script_fulldb import load_word_embeddings, cleaning_for_summarization, get_summary, writeTofile\n",
        "from final_script_fulldb import PreProcess, valid_extensions\n",
        "from main import *\n",
        "from ready_for_search import *\n",
        "def get_text_from_docx_document(file):\n",
        "    try:\n",
        "        doc = Document(file)\n",
        "        temp = ''\n",
        "        for para in doc.paragraphs:\n",
        "            temp += para.text\n",
        "        return temp\n",
        "    except Exception:\n",
        "        print('Raising......')\n",
        "        raise Exception\n",
        "def clean_query(query):\n",
        "    '''\n",
        "    Function to perform lemmatization and cleaning on query\n",
        "    '''\n",
        "    query = re.sub(\"'s\", \"\", query)\n",
        "    query = re.sub(\"s'\", \"\", query)\n",
        "    query = re.sub(\"n't\", \" not\", query)\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word) for word in word_tokenize(query) if word not in STOPWORDS]\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word, pos='v') for word in lemmed]\n",
        "    lemmed = list(set(lemmed))\n",
        "\n",
        "    # applying spell checker on tags\n",
        "    spell = SpellChecker()\n",
        "    misspelled = spell.unknown(lemmed)\n",
        "    new_query = query\n",
        "    if len(misspelled) == 0:\n",
        "        return lemmed, query, new_query\n",
        "    else:\n",
        "        correct_words = list(set(lemmed) - misspelled)\n",
        "        correction = []\n",
        "\n",
        "        for word in misspelled:\n",
        "            # Get the one `most likely` answer\n",
        "            correction.append(spell.correction(word))\n",
        "\n",
        "        for i in range(len(correction)):\n",
        "            new_query = new_query.replace(list(misspelled)[i], correction[i])\n",
        "\n",
        "\n",
        "        # cleaned auto_tags\n",
        "        lemmed = correct_words + correction\n",
        "        print(f\"Searching for {new_query} instead of {query}\")\n",
        "        return lemmed, query, new_query\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['SECRET_KEY'] = 'Hard to guess string'\n",
        "\n",
        "\n",
        "app.config['MAX_CONTENT_LENGTH\t'] = 1024 * 1024 * 1024\n",
        "ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'docx', 'pptx'])\n",
        "\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/searchByTag', methods = ['POST', 'GET'])\n",
        "def viewSearchbyTag():\n",
        "    if request.method == 'POST':\n",
        "        mystring = \"Tag\"\n",
        "        query = request.form['namesearchbytag']\n",
        "\n",
        "        data = pickle.load(open(r\"DataBase/data_file.pkl\", \"rb\"))\n",
        "        titles = pickle.load(open(r\"DataBase/title_file.pkl\", \"rb\"))\n",
        "        auto_tag = pickle.load(open(r\"DataBase/svos_file.pkl\", \"rb\"))\n",
        "        summary = pickle.load(open(r\"DataBase/summary_file.pkl\", \"rb\"))\n",
        "\n",
        "        corpus = pickle.load(open(r\"DataBase/tags_pickle.pkl\", \"rb\"))\n",
        "        bm25 = search_by_BM25(corpus)\n",
        "\n",
        "        tokenized_query, old_query, new_query = clean_query(query.lower())\n",
        "\n",
        "        indexes, results = bm25.get_top_n(tokenized_query, data, n=5)\n",
        "        results_titles = []\n",
        "        results_summaries = []\n",
        "        results_tags = []\n",
        "\n",
        "        for i in indexes:\n",
        "            results_titles.append(titles[i])\n",
        "            results_summaries.append(summary[i])\n",
        "            if auto_tag[i] != []:\n",
        "                results_tags.append(list(set(random.choices(auto_tag[i], k=3))))\n",
        "            else:\n",
        "                results_tags.append(['No Auto tags'])\n",
        "        text = []\n",
        "        for i in results:\n",
        "            text_to_show = \" \".join(sent_tokenize(i)[:2])\n",
        "            if text_to_show != '':\n",
        "                text.append(text_to_show + '....')\n",
        "            else:\n",
        "                text.append(i)\n",
        "        # text = results\n",
        "        title = results_titles\n",
        "        summaries = results_summaries\n",
        "        tags = results_tags\n",
        "\n",
        "        title_len = len(title)\n",
        "\n",
        "        document_file = pickle.load(open(r\"DataBase/document_file.pkl\", \"rb\"))\n",
        "        extension_list = []\n",
        "        for i in indexes:\n",
        "            extension_list.append(document_file[i][\"extension\"])\n",
        "\n",
        "\n",
        "        return render_template('searchbyText.html', text=text, tag=query, title=title, summaries=summaries, tags=tags,\n",
        "                                   type=mystring ,title_len = title_len, old_query=old_query, new_query=new_query,extension_list=extension_list)\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/searchByText', methods = ['POST', 'GET'])\n",
        "def viewSearchbyText():\n",
        "    if request.method == 'POST':\n",
        "        mystring = \"Text\"\n",
        "        query = request.form['namesearchbytext']\n",
        "\n",
        "        data = pickle.load(open(r\"DataBase/data_file.pkl\", \"rb\"))\n",
        "        titles = pickle.load(open(r\"DataBase/title_file.pkl\", \"rb\"))\n",
        "        auto_tag = pickle.load(open(r\"DataBase/svos_file.pkl\", \"rb\"))\n",
        "        summary = pickle.load(open(r\"DataBase/summary_file.pkl\", \"rb\"))\n",
        "\n",
        "        corpus = pickle.load(open(r\"DataBase/corpus_file.pkl\", \"rb\"))\n",
        "        bm25 = search_by_BM25(corpus)\n",
        "\n",
        "        tokenized_query, old_query, new_query = clean_query(query.lower())\n",
        "\n",
        "        indexes, results = bm25.get_top_n(tokenized_query, data, n=5)\n",
        "        results_titles = []\n",
        "        results_summaries = []\n",
        "        results_tags = []\n",
        "\n",
        "        for i in indexes:\n",
        "            results_titles.append(titles[i])\n",
        "            results_summaries.append(summary[i])\n",
        "            if auto_tag[i] != []:\n",
        "                results_tags.append(list(set(random.choices(auto_tag[i], k=3))))\n",
        "            else:\n",
        "                results_tags.append(['No Auto tags'])\n",
        "        text = []\n",
        "        for i in results:\n",
        "            text_to_show = \" \".join(sent_tokenize(i)[:2])\n",
        "            if text_to_show != '':\n",
        "                text.append(text_to_show + '....')\n",
        "            else:\n",
        "                text.append(i)\n",
        "        # text = results\n",
        "        title = results_titles\n",
        "        summaries = results_summaries\n",
        "        tags = results_tags\n",
        "\n",
        "        title_len = len(title)\n",
        "\n",
        "        document_file = pickle.load(open(r\"DataBase/document_file.pkl\", \"rb\"))\n",
        "        extension_list = []\n",
        "        for i in indexes:\n",
        "            extension_list.append(document_file[i][\"extension\"])\n",
        "\n",
        "        # return render_template('searchbyText.html', text=text, tag=query, title=title, summaries=summaries, tags=tags, title_len = title_len)\n",
        "        return render_template('searchbyText.html', text=text, tag=query, title=title, summaries=summaries, tags=tags,\n",
        "                           type=mystring,title_len=title_len, old_query=old_query, new_query=new_query,extension_list=extension_list)\n",
        "\n",
        "\n",
        "@app.route('/searchByTitle', methods=['POST', 'GET'])\n",
        "def viewSearchbyTitle():\n",
        "    if request.method == 'POST':\n",
        "        mystring = \"Title\"\n",
        "        query = request.form['namesearchbytitle']\n",
        "        data = pickle.load(open(r\"DataBase/data_file.pkl\", \"rb\"))\n",
        "        titles = pickle.load(open(r\"DataBase/title_file.pkl\", \"rb\"))\n",
        "        auto_tag = pickle.load(open(r\"DataBase/svos_file.pkl\", \"rb\"))\n",
        "        summary = pickle.load(open(r\"DataBase/summary_file.pkl\", \"rb\"))\n",
        "\n",
        "        corpus = pickle.load(open(r\"DataBase/title_corpus.pkl\", \"rb\"))\n",
        "        bm25 = search_by_BM25(corpus)\n",
        "\n",
        "        tokenized_query, old_query, new_query = clean_query(query.lower())\n",
        "\n",
        "        indexes, results = bm25.get_top_n(tokenized_query, data, n=5)\n",
        "        results_titles = []\n",
        "        results_summaries = []\n",
        "        results_tags = []\n",
        "\n",
        "        for i in indexes:\n",
        "            results_titles.append(titles[i])\n",
        "            results_summaries.append(summary[i])\n",
        "            if auto_tag[i] != []:\n",
        "                results_tags.append(list(set(random.choices(auto_tag[i], k=3))))\n",
        "            else:\n",
        "                results_tags.append(['No Auto tags'])\n",
        "        text = []\n",
        "        for i in results:\n",
        "            text_to_show = \" \".join(sent_tokenize(i)[:2])\n",
        "            if text_to_show != '':\n",
        "                text.append(text_to_show + '....')\n",
        "            else:\n",
        "                text.append(i)\n",
        "\n",
        "        # text = results\n",
        "        title = results_titles\n",
        "        summaries = results_summaries\n",
        "        tags = results_tags\n",
        "\n",
        "        title_len = len(title)\n",
        "\n",
        "        document_file = pickle.load(open(r\"DataBase/document_file.pkl\", \"rb\"))\n",
        "        extension_list = []\n",
        "        for i in indexes:\n",
        "            extension_list.append(document_file[i][\"extension\"])\n",
        "\n",
        "        # return render_template('searchbyTitle.html', text=text, tag=query, title=title, summaries=summaries, tags=tags, title_len = title_len)\n",
        "        return render_template('searchbyText.html', text=text, tag=query, title=title, summaries=summaries, tags=tags,\n",
        "                                type=mystring,title_len=title_len, old_query=old_query, new_query=new_query,extension_list=extension_list)\n",
        "\n",
        "\n",
        "@app.route('/', methods=['POST'])\n",
        "def upload_file():\n",
        "    if request.method == 'POST':\n",
        "        if 'files[]' not in request.files:\n",
        "            return redirect(request.url)\n",
        "        files = request.files.getlist(r'files[]')\n",
        "        print(files[0].filename)\n",
        "        file_upload = files[0].filename\n",
        "\n",
        "        # taking filename as a title\n",
        "        title = \" \".join(file_upload.split('.')[:-1])\n",
        "\n",
        "        try:\n",
        "            for file in files:\n",
        "                if file and allowed_file(file.filename):\n",
        "                    filename = secure_filename(file.filename)\n",
        "                    file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "\n",
        "            # go to that file and read it\n",
        "            file_upload = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "            print(file_upload)\n",
        "            main(file_upload, title)\n",
        "            # after completion of processsing delete that file from folder.\n",
        "            os.remove(file_upload)\n",
        "\n",
        "            # I know this way of doing it, is very wrong, It's more like a cheating. But I have done this for a particular reason\n",
        "            # I will change it after sometime.\n",
        "\n",
        "            return redirect('/')\n",
        "\n",
        "        except Exception:\n",
        "            print(\"Hello\")\n",
        "            return redirect('/')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "var_path = \"\"\n",
        "@app.route('/path', methods=['POST'])\n",
        "def choose():\n",
        "    app.config['UPLOAD_FOLDER'] = \"\"\n",
        "    global var_path\n",
        "    var_path = request.form.get('folder_path')\n",
        "\n",
        "    print(var_path)\n",
        "    app.config['UPLOAD_FOLDER'] = var_path\n",
        "    #print(app.config['UPLOAD_FOLDER'])\n",
        "\n",
        "    return redirect('/')\n",
        "\n",
        "\n",
        "@app.route('/nopage')\n",
        "def noaccountpagefunction():\n",
        "    return render_template('nopage.html')\n",
        "\n",
        "\n",
        "# tempdiv = \"\"\n",
        "myclassname = \"\"\n",
        "\n",
        "\n",
        "@app.route('/filenameonclick', methods=['GET', 'POST'])\n",
        "def filenameonclick():\n",
        "    if request.method == 'POST':\n",
        "        if os.path.exists(var_path):\n",
        "\n",
        "            myclassname = request.form['myclassname']\n",
        "            print(myclassname)\n",
        "            titles = pickle.load(open(r\"DataBase/title_file.pkl\", \"rb\"))\n",
        "            for i in range(len(titles)):\n",
        "                if titles[i] == myclassname:\n",
        "                    index = i\n",
        "                    break\n",
        "\n",
        "            document_file = pickle.load(open(r\"DataBase/document_file.pkl\", \"rb\"))\n",
        "\n",
        "            blob_data = document_file[index][\"document\"]\n",
        "            extension = document_file[index][\"extension\"]\n",
        "\n",
        "            if len(myclassname) > 80:\n",
        "                myclassname = myclassname[:80]\n",
        "\n",
        "            punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'\n",
        "            for x in myclassname:\n",
        "                if x in punctuations:\n",
        "                    myclassname = myclassname.replace(x, \"\")\n",
        "\n",
        "            print(myclassname)\n",
        "            file_name = myclassname + '.' + extension\n",
        "            file_name = os.path.join(var_path, file_name)\n",
        "\n",
        "            writeTofile(blob_data, file_name)\n",
        "\n",
        "            return render_template('redirect.html', myclassname=myclassname)\n",
        "        else:\n",
        "            mymessage = \"Please enter the working directory for current session.\"\n",
        "            return render_template('checkworking.html' , mymessage=mymessage)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIofgDjg1eTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NASms7e1d0S",
        "colab_type": "text"
      },
      "source": [
        "main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6mE_GiB1dhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3\n",
        "import os\n",
        "import pickle\n",
        "from auto_tagging_script import SubjectTrigramTagger\n",
        "from auto_tagging_script import AutoTags\n",
        "\n",
        "from final_script_fulldb import load_word_embeddings, cleaning_for_summarization, get_summary\n",
        "from final_script_fulldb import PreProcess, valid_extensions\n",
        "from ready_for_search import *\n",
        "print('imported')\n",
        "\n",
        "def convertToBinaryData(file):\n",
        "    #Convert digital data to binary format\n",
        "    with open(file, 'rb') as file:\n",
        "        blobData = file.read()\n",
        "    return blobData\n",
        "\n",
        "\n",
        "def insert_data_to_database(doc_id, title, text, file, extension, summary, auto_tags, manual_tags, svos):\n",
        "    try:\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "\n",
        "        sqlite_insert_blob_query1 = \"\"\" INSERT INTO document_info\n",
        "                                              (doc_id, title, text, document,extension) VALUES (?, ?, ?, ?, ?)\"\"\"\n",
        "\n",
        "        document = convertToBinaryData(file)\n",
        "        # Convert data into tuple format\n",
        "        data_tuple1 = (doc_id, title, text, document, extension)\n",
        "        c.execute(sqlite_insert_blob_query1, data_tuple1)\n",
        "\n",
        "        sqlite_insert_blob_query2 = \"\"\" INSERT INTO document_summary\n",
        "                                              (doc_id, summary) VALUES (?, ?)\"\"\"\n",
        "\n",
        "        # Convert data into tuple format\n",
        "        data_tuple2 = (doc_id, summary)\n",
        "        c.execute(sqlite_insert_blob_query2, data_tuple2)\n",
        "\n",
        "        sqlite_insert_blob_query3 = \"\"\" INSERT INTO document_tags\n",
        "                                          (doc_id, title, auto_tags, manual_tags,svos) VALUES (?, ?, ?, ?, ?)\"\"\"\n",
        "\n",
        "        # Convert data into tuple format\n",
        "        data_tuple3 = (doc_id, title, auto_tags, manual_tags, svos)\n",
        "        c.execute(sqlite_insert_blob_query3, data_tuple3)\n",
        "\n",
        "        conn.commit()\n",
        "        print(\"file and data inserted successfully into a table\")\n",
        "        conn.close()\n",
        "\n",
        "        # call maintaining_all_files() fn for updating all files for search.\n",
        "        maintaining_all_files()\n",
        "\n",
        "    except sqlite3.Error as error:\n",
        "        conn.rollback()\n",
        "        print(\"Failed to insert data into sqlite table\", error)\n",
        "        raise Exception\n",
        "    finally:\n",
        "        if (conn):\n",
        "            conn.close()\n",
        "            # print(\"the sqlite connection is closed\")\n",
        "\n",
        "def get_last_inserted_rowid():\n",
        "    try:\n",
        "        conn = sqlite3.connect(r\"DataBase/Document_finder_db2.db\")\n",
        "        c = conn.cursor()\n",
        "        c.execute('''SELECT MAX(rowid) FROM document_info''')\n",
        "        tup = c.fetchone()\n",
        "        conn.close()\n",
        "        return tup[0]\n",
        "    except Exception:\n",
        "        print('Cannot access the database right now')\n",
        "\n",
        "def main(file_upload, title):\n",
        "\n",
        "    # load_word_embeddings()\n",
        "    # print('loaded')\n",
        "    global word_embeddings\n",
        "    word_embeddings = pickle.load(open(r\"word_embeddings.json\", \"rb\"))\n",
        "    \n",
        "    preprocess_obj = PreProcess(file_upload)\n",
        "\n",
        "    if preprocess_obj.check_extension():\n",
        "\n",
        "        extension = preprocess_obj.get_extension()\n",
        "\n",
        "        if extension == 'docx':\n",
        "            text = preprocess_obj.get_text_from_docx_document()\n",
        "            text = preprocess_obj.remove_escape_sequences(text)\n",
        "            \n",
        "        elif extension == 'pptx':\n",
        "            text = preprocess_obj.get_text_from_pptx_document()\n",
        "            text = preprocess_obj.remove_escape_sequences(text)\n",
        "\n",
        "        elif extension == 'pdf':\n",
        "            text = preprocess_obj.get_text_from_pdf_document()\n",
        "            text = preprocess_obj.remove_escape_sequences(text)\n",
        "\n",
        "        else:\n",
        "            text = preprocess_obj.get_text_from_txt_document()\n",
        "            text = preprocess_obj.remove_escape_sequences(text)\n",
        "\n",
        "        #doc_id = str(file_upload.split('\\\\')[-1]).replace('.' + extension, \"\")  # name of file(in local directory) as doc_id\n",
        "\n",
        "        # title = data.title[int(re.findall(\"_[0-9]+\",doc_id)[0][1:])-1]\n",
        "\n",
        "        doc_id = f'news_{get_last_inserted_rowid()+15}'                # doc_id = 'news_5782'\n",
        "        # doc_id = 'news_5783'\n",
        "        print(doc_id)\n",
        "        # title = input(\"Enter Title\")\n",
        "        # text = text\n",
        "        print(text)\n",
        "        summary = get_summary(text, word_embeddings)\n",
        "        print(summary)\n",
        "\n",
        "        # manual_tags = str(list(map(str, input(\"Enter manual tags\").split(\"  \"))))\n",
        "        manual_tags = \"\"\n",
        "        auto_tags_obj = AutoTags()\n",
        "        auto_tags, svos = auto_tags_obj.get_auto_tags_from_document(text, doc_id)\n",
        "\n",
        "        assert type(auto_tags) == type(svos) == str, r\"tags cannot be inserted into table as its data type doesn't match the database's data type\"\n",
        "\n",
        "        print(auto_tags)\n",
        "        print(manual_tags)\n",
        "\n",
        "        insert_data_to_database(doc_id, title, text, file_upload, extension, summary, auto_tags, manual_tags, svos)\n",
        "\n",
        "\n",
        "    else:\n",
        "        print('Invalid Extension')\n",
        "\n",
        "# main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0yWa5N72Ga7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q86Bppy2Gvw",
        "colab_type": "text"
      },
      "source": [
        "Document Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvgL-iEx2HPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import LdaModel\n",
        "from gensim import models, similarities\n",
        "import numpy as np\n",
        "from gensim.corpora import Dictionary\n",
        "import time\n",
        "import pickle\n",
        "from scipy.stats import entropy\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "print(\"Imported\")\n",
        "corpus = pickle.load(open(r\"Database\\corpus_file.pkl\", \"rb\"))\n",
        "\n",
        "\n",
        "def train_lda(corpus):\n",
        "    num_topics = 100\n",
        "    chunksize = 1000\n",
        "    dictionary = Dictionary(corpus)\n",
        "    lda_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
        "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
        "\n",
        "    lda = LdaModel(corpus=lda_corpus, num_topics=num_topics, id2word=dictionary,\n",
        "                   alpha=1e-2, eta=5e-2, chunksize=chunksize,\n",
        "                   minimum_probability=0.0, passes=2)\n",
        "\n",
        "    t2 = time.time()\n",
        "    print(\"Time to train LDA model on \", len(corpus), \"articles\", (t2 - t1) / 60, \"min\")\n",
        "    return dictionary, lda_corpus, lda\n",
        "\n",
        "\n",
        "def jensen_shannon(query, matrix):\n",
        "    p = query[:, None]  # original shape of query was (100,) , which means ---> (number of topics,)\n",
        "    print(p.shape)  # shape becomes (100,1)\n",
        "\n",
        "    q = matrix.T  # transpose matrix\n",
        "    print(q.shape)  # shape --> (number of topics, total documents)\n",
        "    return jensenshannon(p, q)\n",
        "\n",
        "    # m = 0.5 * (p + q)\n",
        "    # return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))\n",
        "\n",
        "\n",
        "def execute_training_of_lda(corpus):\n",
        "    dictionary, lda_corpus, lda = train_lda(corpus)\n",
        "\n",
        "    t3 = time.time()\n",
        "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[lda_corpus]])\n",
        "    t4 = time.time()\n",
        "\n",
        "    print(\"Time to get topic distribution\", (t4 - t3) / 60, \"min\")\n",
        "    pickle.dump(dictionary, open(r'LdaModel/dictionary.pkl', \"wb\"))\n",
        "    pickle.dump(doc_topic_dist, open(r'LdaModel/doc_topic_distribution.pkl', \"wb\"))\n",
        "    lda.save(r'LdaModel/model')\n",
        "    print(\"All files are ready----\")\n",
        "\n",
        "def get_most_similar_documents(query,matrix,k=10):\n",
        "    \"\"\"\n",
        "    This function implements the Jensen-Shannon distance above\n",
        "    and retruns the top k indices of the smallest jensen shannon distances\n",
        "    \"\"\"\n",
        "    sims = jensen_shannon(query, matrix)    # list of jensen shannon distances\n",
        "    print(max(sims))\n",
        "    print(sorted(sims, reverse=True)[:10])\n",
        "    return sims.argsort()[:k]   # the top k positional index of the smallest Jensen Shannon distances\n",
        "\n",
        "def get_similar_documents(doc_corpus):\n",
        "    dictionary = pickle.load(open(r\"LdaModel/dictionary.pkl\", \"rb\"))\n",
        "    doc_topic_dist = pickle.load(open(r\"LdaModel/doc_topic_distribution.pkl\", \"rb\"))\n",
        "    lda = LdaModel.load(r\"LdaModel/model\")\n",
        "    titles = pickle.load(open(r\"DataBase/title_file.pkl\", \"rb\"))\n",
        "\n",
        "    bow = dictionary.doc2bow(doc_corpus)\n",
        "    doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n",
        "\n",
        "    most_sim_ids = get_most_similar_documents(doc_distribution, doc_topic_dist)\n",
        "    return most_sim_ids\n",
        "    # print()\n",
        "    # for i in most_sim_ids:\n",
        "    #     print(titles[i])\n",
        "    #     print(\"===================\")\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#\n",
        "#     # t1 = time.time()\n",
        "#     # print(\"Starting training...\")\n",
        "#     # execute_training_of_lda(corpus)\n",
        "#     # t2 = time.time()\n",
        "#     # print(\"Total time taken: \", (t2-t1)/60, \"min\")\n",
        "#\n",
        "#     doc_corpus = corpus[78]\n",
        "#     get_similar_documents(doc_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa0SRV0H2MWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdR0rdQ2Mrv",
        "colab_type": "text"
      },
      "source": [
        "check.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvCmKni2Zs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = int(input())\n",
        "arr = list(map(int, input().split()))\n",
        "\n",
        "\n",
        "def func(n, arr):\n",
        "    global swaps\n",
        "    swaps = 0\n",
        "    for i in range(n-1, 1, -1):\n",
        "        if arr[i] == i + 1:\n",
        "            continue\n",
        "        elif arr[i - 1] == i + 1:\n",
        "            arr[i], arr[i - 1] = arr[i - 1], arr[i]\n",
        "            swaps += 1\n",
        "        elif arr[i - 2] == i + 1:\n",
        "            arr[i - 1], arr[i - 2] = arr[i - 2], arr[i - 1]\n",
        "            arr[i], arr[i - 1] = arr[i - 1], arr[i]\n",
        "            swaps += 2\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    if arr[1] == 2:\n",
        "        return True\n",
        "    elif arr[0] == 2 and arr[1] == 1:\n",
        "        swaps += 1\n",
        "        arr[0], arr[1] = arr[0], arr[1]\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "if func(n, arr):\n",
        "    print(\"YES\")\n",
        "    print(swaps)\n",
        "else:\n",
        "    print(\"NO\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dgdnQYW2hJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP9jJY4B2hjO",
        "colab_type": "text"
      },
      "source": [
        "Project.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiOI43c62jMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tkinter import filedialog\n",
        "import tkinter as tk\n",
        "import tkinter.messagebox\n",
        "from app import *\n",
        "from main import *\n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "def on_click(event):\n",
        "    filepath.configure(state=tk.NORMAL)\n",
        "    filepath.delete(0, tk.END)\n",
        "\n",
        "    # make the callback only work once\n",
        "    filepath.unbind('<Button-1>', on_click_id)\n",
        "\n",
        "\n",
        "def first_window():\n",
        "    global window0\n",
        "    window0 = tk.Tk()\n",
        "    text2 = tk.Text(window0, height=25, width=150,)\n",
        "    scroll = tk.Scrollbar(window0, command=text2.yview)\n",
        "    text2.configure(yscrollcommand=scroll.set)\n",
        "    text2.tag_configure('bold_italics', font=('Arial', 20, 'bold', 'italic'))\n",
        "    text2.tag_configure('big', font=('Verdana', 20, 'bold'))\n",
        "    text2.tag_configure('color',\n",
        "                        foreground='#476042',\n",
        "                        font=('Tempus Sans ITC', 12, 'bold'))\n",
        "\n",
        "\n",
        "    text2.insert(tk.END, '\\nDocument Search Manager\\n', 'big')\n",
        "    quote = \"\"\"\n",
        "    How easy do you find it to remember the exact location of a document that you created last year?\n",
        "    Not very easy, right?\n",
        "    Big Organizations/people deal with hundreds of documents daily and forget about them, most of the time. \n",
        "    But what if we want that old documentation again for some work,\n",
        "    but unfortunately you do not remember the name or the actual content of that document to retrieve it from the large storage of your computer. \n",
        "    In such cases, use of a Intelligent document finder can really make a huge difference.\n",
        "    \"\"\"\n",
        "    text2.insert(tk.END, quote, 'color')\n",
        "    labelfont = ('times', 40, 'bold')\n",
        "    text2.config(bg='black', fg='yellow')\n",
        "    text2.config(font=labelfont)\n",
        "    text2.config(height=10, width=40)\n",
        "    text2.pack(expand=True, fill=tk.BOTH)\n",
        "\n",
        "    text2.pack(fill=tk.BOTH, anchor=\"e\")\n",
        "    scroll.pack(fill=tk.Y)\n",
        "\n",
        "    contentfont = ('times', 20, 'bold')\n",
        "    contentframe = tk.Label(window0, text=\"Do you want to upload a file or search for a file?\")\n",
        "    contentframe.config(font=contentfont)\n",
        "    contentframe.config(bg='black', fg='aqua')\n",
        "    contentframe.pack(expand=True, fill=tk.BOTH)\n",
        "    contentframe.pack()\n",
        "\n",
        "    uploadbutton=tk.Button(window0, text=\"Upload file\", relief=tk.GROOVE, borderwidth=5, command=upload_window)\n",
        "    uploadfont = ('times', 25, 'bold')\n",
        "    uploadbutton.config(font=uploadfont)\n",
        "    uploadbutton.config(bg='black', fg='aqua')\n",
        "    uploadbutton.pack(side= tk.LEFT, expand=True, fill=tk.BOTH, padx=15, pady=15)\n",
        "\n",
        "    searchbutton=tk.Button(window0, text=\"Search file\", relief=tk.GROOVE, borderwidth=5, command=main_func)\n",
        "    searchfont = ('times', 25, 'bold')\n",
        "    searchbutton.config(font=searchfont)\n",
        "    searchbutton.config(bg='black', fg='aqua')\n",
        "    searchbutton.pack(side=tk.RIGHT, expand=True, fill=tk.BOTH, padx=15, pady=15)\n",
        "\n",
        "    window0.mainloop()\n",
        "def print_path():\n",
        "    global title, f\n",
        "    f = tk.filedialog.askopenfilename(\n",
        "        parent=window, initialdir='C:/Tutorial',\n",
        "        title='Choose file',\n",
        "        filetypes=[('txt files', '.txt'),\n",
        "                   ('all files', '*')]\n",
        "    )\n",
        "\n",
        "    filename = os.path.basename(f)\n",
        "    fileupload.config(state=tk.NORMAL)\n",
        "    fileupload.insert(0, filename)\n",
        "    fileupload.config(state=tk.DISABLED)\n",
        "    print(f)\n",
        "    title = \".\".join(filename.split('.')[:-1])\n",
        "    print(title)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def delete_path():\n",
        "    fileupload.config(state=tk.NORMAL)\n",
        "    fileupload.delete(0, tk.END)\n",
        "    fileupload.config(state=tk.DISABLED)\n",
        "\n",
        "\n",
        "\n",
        "def upload_file():\n",
        "\n",
        "    var_path = filepath.get()\n",
        "    flag = True\n",
        "    if not os.path.exists(var_path):\n",
        "        tk.messagebox.showerror(title='Path Error', message='Entered Directory does not exists!! Please Enter Valid Path')\n",
        "        filepath.delete(0, tk.END)\n",
        "        flag = False\n",
        "    try:\n",
        "        if not os.path.exists(f):\n",
        "            print('f: ',f)\n",
        "            tk.messagebox.showerror(title='File Not Found', message='Please Upload Correct File on System first!! By click on upload file button')\n",
        "            flag = False\n",
        "        else:\n",
        "            if f.split('.')[-1] not in ALLOWED_EXTENSIONS:\n",
        "                print(f)\n",
        "                tk.messagebox.showerror(title='Error', message='File Format is not Supported!! Please upload files of only docx,txt,pdf or pptx format')\n",
        "                flag = False\n",
        "    except NameError:\n",
        "        tk.messagebox.showerror(title='Error',\n",
        "                                message='You have not choosen any file yet!!')\n",
        "        flag = False\n",
        "\n",
        "    if flag == True:\n",
        "        with open(\"path.txt\", \"w\") as file:\n",
        "            file.write(var_path)\n",
        "\n",
        "        main(f, title)\n",
        "        tk.messagebox.showinfo(\"Successful\", \"File Successfully uploaded!\")\n",
        "\n",
        "\n",
        "# def main_func2():\n",
        "#     # p1 = subprocess.run(['python', 'app.py'], shell=True)\n",
        "#     # p1.terminate()\n",
        "#     # p1.kill()\n",
        "#     p2 = multiprocessing.Process(target=main_func())\n",
        "#     p2.run()\n",
        "#     # p2.start()\n",
        "#     # sleep(5)\n",
        "#     # p2.terminate()\n",
        "#\n",
        "#     # p2.join()\n",
        "#\n",
        "placeholder = \"Enter Working Directory of this System\"\n",
        "\n",
        "\n",
        "def upload_window():\n",
        "    global window\n",
        "    window = tk.Toplevel(window0)\n",
        "    # window = tk.Tk()\n",
        "    # window0.withdraw()\n",
        "    window.title(\"Upload File\")\n",
        "\n",
        "    titleframe = tk.Label(window, text=\"Click on the Upload file button and browse the file that you want to upload.\",\n",
        "                          relief=tk.RAISED)\n",
        "    labelfont = ('times', 20, 'bold')\n",
        "    titleframe.config(bg='black', fg='yellow')\n",
        "    titleframe.config(font=labelfont)\n",
        "    titleframe.config(height=10, width=80)\n",
        "    titleframe.pack(expand=True, fill=tk.BOTH)\n",
        "    titleframe.pack()\n",
        "\n",
        "    fp = tk.Frame(window)\n",
        "    fp.pack()\n",
        "\n",
        "    global filepath, on_click_id, var_path\n",
        "    filepath = tk.Entry(fp, font=(\"Times New Roman\", 15, \"bold\"), justify='center')\n",
        "    filepath.config(width=120, bd=5, relief=tk.SUNKEN)\n",
        "    filepath.insert(0, placeholder)\n",
        "    filepath.config(state=tk.DISABLED)\n",
        "\n",
        "    on_click_id = filepath.bind('<Button-1>', on_click)\n",
        "    # path = filepath.get()\n",
        "    filepath.pack(side=tk.LEFT)\n",
        "\n",
        "    # var_path = path\n",
        "\n",
        "    buttons = tk.Frame(window)\n",
        "    buttons.pack(side=tk.LEFT)\n",
        "\n",
        "    uploadbutton2 = tk.Button(buttons, text='Upload file', command=print_path, width=20)\n",
        "    buttonfont = ('times', 25, 'bold')\n",
        "    uploadbutton2.config(font=buttonfont)\n",
        "    uploadbutton2.config(bg='black', fg='aqua')\n",
        "    uploadbutton2.pack(expand=True, fill=tk.BOTH, padx=15, pady=15)\n",
        "\n",
        "    removefile = tk.Button(buttons, text=\"Remove file\", command=delete_path, relief=tk.GROOVE, borderwidth=5, width=20)\n",
        "    removefont = ('times', 25, 'bold')\n",
        "    removefile.config(font=removefont)\n",
        "    removefile.config(bg='black', fg='aqua')\n",
        "    removefile.pack(side=tk.LEFT, expand=True, fill=tk.BOTH, padx=15, pady=15)\n",
        "\n",
        "    upent = tk.Frame(window)\n",
        "    upent.pack()\n",
        "    global fileupload\n",
        "    fileupload = tk.Entry(upent, width=60, bd=5, font=(\"Times New Roman\", 14, \"bold\"), justify='center',\n",
        "                          state=tk.DISABLED)\n",
        "    fileupload.config(fg='brown')\n",
        "    fileupload.pack(side=tk.LEFT)\n",
        "\n",
        "    upbutton = tk.Button(upent, text=\"Upload\", command=upload_file, relief=tk.GROOVE, borderwidth=5)\n",
        "    uploadfont2 = ('times', 25, 'bold')\n",
        "    upbutton.config(font=uploadfont2)\n",
        "    upbutton.config(bg='black', fg='aqua')\n",
        "    upbutton.pack(expand=True, fill=tk.BOTH, padx=15, pady=15)\n",
        "\n",
        "    # window.mainloop()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# p1 = multiprocessing.Process(target=first_window())\n",
        "# # p2 = multiprocessing.Process(target=app.run)\n",
        "# #\n",
        "# p1.start()\n",
        "# #\n",
        "# # # p2. join()\n",
        "# p1.join()\n",
        "first_window()\n",
        "# upload_window()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiaSXjj22kyP",
        "colab_type": "text"
      },
      "source": [
        "Data preparation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgIBAydH21Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim import models, corpora, similarities\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import time\n",
        "from nltk import FreqDist\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gRzxips25yM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyK2EIIk26Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table[table.duplicated(subset=['Title'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQvm75f02-Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table[table.Title==\"Revenue\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_czN76JF3Izv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table.drop_duplicates(subset=['Title'],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssCcz40i3MAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = table[table.Text.map(len)>500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYcxvYqz3TKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# confirming\n",
        "table[table.Text.map(len)<500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDiq7ht33TGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table['Table Table'] = table['Table Table'].map(str)\n",
        "\n",
        "table.Text = table.Text.str.strip()\n",
        "table['Table Table'] = table['Table Table'].str.strip()\n",
        "\n",
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function to clean text of websites, email addresess and any punctuation\n",
        "    We also lower case the text\n",
        "    \"\"\"\n",
        "    pattern = r\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\"\n",
        "    text = re.sub('[[a-zA-Z0-9]+]|[\\n[\\nedit\\n]+]',' ',text)\n",
        "    text = re.sub(pattern,\" \",text)\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    text = text.lower()\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "stop_words = STOPWORDS\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    '''\n",
        "    Function to remove stopwords from text\n",
        "    '''\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "def lemmatize(text):\n",
        "    '''\n",
        "    Function to perform lemmatization on text\n",
        "    '''\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word) for word in text if len(word)>2]\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word,pos='v') for word in lemmed]\n",
        "    return lemmed\n",
        "\n",
        "def apply_all(text):\n",
        "    return lemmatize(remove_stop_words(initial_clean(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMBE6tc63TFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean text and title and create new column \"tokenized\"\n",
        "t1 = time.time()\n",
        "table['tokenized'] = table['Text'].apply(apply_all) + table['Table Table'].apply(apply_all)\n",
        "t2 = time.time()\n",
        "print(\"Time to clean and tokenize\", len(table), \"articles:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XjImZil3nD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of all words\n",
        "all_words = [word for item in list(table['tokenized']) for word in item]\n",
        "# use nltk fdist to get a frequency distribution of all words\n",
        "fdist = FreqDist(all_words)\n",
        "len(fdist) # number of unique words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r530a6R13nLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(fdist,open(\"fdist_file2.pkl\",\"wb\"))\n",
        "\n",
        "fdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tpIG_GQ3nHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 100000\n",
        "top_k_words = fdist.most_common(k)\n",
        "top_k_words[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZXfIpNf3nBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to keep only top k words\n",
        "top_k_words,_ = zip(*fdist.most_common(k))\n",
        "top_k_words = set(top_k_words)\n",
        "\n",
        "def keep_top_k_words(text):\n",
        "    return [word for word in text if word in top_k_words]\n",
        "\n",
        "table['tokenized'] = table['tokenized'].apply(keep_top_k_words)\n",
        "\n",
        "# document length\n",
        "table['doc_len'] = table['tokenized'].apply(lambda x: len(x))\n",
        "doc_lengths = list(table['doc_len'])\n",
        "table.drop(labels='doc_len', axis=1, inplace=True)\n",
        "\n",
        "print(\"length of list:\",len(doc_lengths),\n",
        "      \"\\naverage document length\", np.average(doc_lengths),\n",
        "      \"\\nminimum document length\", min(doc_lengths),\n",
        "      \"\\nmaximum document length\", max(doc_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4h14v5L38HL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(table['tokenized'])\n",
        "\n",
        "pickle.dump(corpus,open(\"wiki_corpus_file.pkl\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKK1k61d37PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_title_file = list(table[\"Title\"])\n",
        "wiki_text_file = list(table[\"Text\"])\n",
        "\n",
        "wiki_title_file[10]\n",
        "\n",
        "'Severe acute respiratory syndrome coronavirus 2'\n",
        "\n",
        "pickle.dump(wiki_title_file,open(\"wiki_title_file.pkl\",\"wb\"))\n",
        "pickle.dump(wiki_text_file,open(\"wiki_text_file.pkl\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0OsZsTD4JKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = pd.read_csv(\"wikipedia_articles2.csv\")\n",
        "\n",
        "table.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wu2uwVI4JHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# table[table.duplicated(subset=['Title','Text'])]\n",
        "table.drop_duplicates(subset=['Title','Text'],inplace=True)\n",
        "\n",
        "\n",
        "# table[table.Text.map(len)<400]\n",
        "# print(table.Text[3642])\n",
        "print(table.URL[4759])\n",
        "print(table.Text[8851])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agVfI2KX4SQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcWoo2ke4SYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(table.Text[0].strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cByHMBLy4SWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table['Table Text'] = table['Table Text'].map(str)\n",
        "table['Table Text'] = table['Table Text'].str.strip()\n",
        "\n",
        "table.Text = table.Text.str.strip()\n",
        "table.Text = table.Text.map(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSbMgU5I4SN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAMdtO_k4cCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function to clean text which can contain Links, email addresess, and any punctuation\n",
        "    We also lower case the text\n",
        "    \"\"\"\n",
        "    pattern = r\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\"\n",
        "    text = re.sub(pattern,\" \",text)\n",
        "    text = re.sub('[[a-zA-Z0-9]+]|[\\n[\\nedit\\n]+]',' ',text)\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    text = text.lower()\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "stop_words = STOPWORDS\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    '''\n",
        "    Function to remove stopwords from text\n",
        "    '''\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "def lemmatize(text):\n",
        "    '''\n",
        "    Function to perform lemmatization on text\n",
        "    '''\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word) for word in text if len(word)>2]\n",
        "    lemmed = [WordNetLemmatizer().lemmatize(word,pos='v') for word in lemmed]\n",
        "    return lemmed\n",
        "\n",
        "def apply_all(text):\n",
        "    return lemmatize(remove_stop_words(initial_clean(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfzlF2kS4b-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RX_tLX-4b8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean text and title and create new column \"tokenized\"\n",
        "t1 = time.time()\n",
        "table['tokenized'] = table['Text'].apply(apply_all) + table['Table Table'].apply(apply_all)\n",
        "t2 = time.time()\n",
        "print(\"Time to clean and tokenize\", len(table), \"articles:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}